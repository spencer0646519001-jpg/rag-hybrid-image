# search_day9.py
from __future__ import annotations
import re
from typing import List, Dict, Tuple
import numpy as np

# ---------- åŸºæœ¬æ¸…æ´— ----------
def normalize_text(s: str) -> str:
    s = (s or "").lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s

def tokenize(s: str) -> List[str]:
    s = normalize_text(s)
    # åªä¿ç•™ a-z çš„ç°¡æ˜“æ–·è©ï¼›è¦æ”¯æ´ä¸­æ—¥æ–‡å¯æ›åˆ¥çš„ tokenizer
    return re.findall(r"[a-z]+", s)

# ---------- æº–å‚™ä¸€é»ç¤ºä¾‹è³‡æ–™ï¼ˆtitle / snippet / tagsï¼‰ ----------
DOCS: List[Dict] = [
    {"title":"Opera Cake",
     "snippet":"French coffee buttercream and chocolate glaze cake.",
     "tags":["coffee","almond","buttercream"]},
    {"title":"Tiramisu",
     "snippet":"Italian dessert with espresso and mascarpone.",
     "tags":["coffee","cocoa"]},
    {"title":"Matcha Mousse",
     "snippet":"Light Japanese dessert with matcha green tea.",
     "tags":["matcha","tea","mousse"]},
    {"title":"Mont Blanc",
     "snippet":"Chestnut cream dessert, often with meringue.",
     "tags":["chestnut","cream"]},
    {"title":"Chocolate Tart",
     "snippet":"Rich chocolate ganache with crisp tart shell.",
     "tags":["chocolate","ganache"]},
]

# ---------- å»º vocabï¼ˆç”¨æ‰€æœ‰æ–‡ä»¶çš„æ–‡å­—ï¼‰ ----------
def build_vocab(docs: List[Dict]) -> Tuple[List[str], Dict[str,int]]:
    bag = []
    for d in docs:
        bag += tokenize(d["title"])
        bag += tokenize(d["snippet"])
        bag += [normalize_text(t) for t in d.get("tags", [])]
    vocab = sorted(set(bag))
    token2id = {tok:i for i, tok in enumerate(vocab)}
    return vocab, token2id

VOCAB, TOK2ID = build_vocab(DOCS)

# ---------- å‡ embeddingï¼šBag-of-Words â†’ L2 è¦ä¸€åŒ– ----------
def vectorize(text: str, tok2id: Dict[str,int]) -> np.ndarray:
    vec = np.zeros(len(tok2id), dtype=float)
    for tok in tokenize(text):
        if tok in tok2id:
            vec[tok2id[tok]] += 1.0            # ç°¡å–®è©é » TF
    # L2 normalizationï¼šåªæ¯”æ–¹å‘
    norm = np.linalg.norm(vec)
    return vec if norm == 0 else vec / norm

def doc_to_text(d: Dict) -> str:
    return f'{d["title"]} {d["snippet"]} {" ".join(d.get("tags", []))}'

def build_doc_matrix(docs: List[Dict]) -> np.ndarray:
    mat = []
    for d in docs:
        mat.append(vectorize(doc_to_text(d), TOK2ID))
    return np.vstack(mat) if mat else np.zeros((0, len(TOK2ID)))

DOC_MATRIX = build_doc_matrix(DOCS)   # shape: (n_docs, |V|)

# ---------- æ ¸å¿ƒï¼šsearch(query) ----------
def search(query: str, top_k: int = 3) -> List[Tuple[int, float]]:
    qv = vectorize(query, TOK2ID)            # (|V|,)
    # é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆå› ç‚ºéƒ½ L2 è¦ä¸€åŒ–ï¼Œå…§ç©å°±æ˜¯ cosÎ¸ï¼‰
    sims = DOC_MATRIX @ qv                   # (n_docs,)
    top_idx = np.argsort(-sims)[:top_k]      # å–åˆ†æ•¸é«˜çš„å‰ k
    return [(int(i), float(sims[i])) for i in top_idx]

# ---------- Demo ----------
if __name__ == "__main__":
    q = "coffee cake"
    results = search(q, top_k=3)
    print(f'ğŸ” Query: "{q}"')
    for rank, (i, s) in enumerate(results, 1):
        d = DOCS[i]
        print(f"{rank}. {d['title']:15s}  score={s:.3f}  | {d['snippet']}")

if __name__ == "__main__":
    q = "coffee cake"
    results = search(q, top_k=3)
    print(f'ğŸ” Query: "{q}"')
    for rank, (i, s) in enumerate(results, 1):
        d = DOCS[i]
        print(f"{rank}. {d['title']:15s}  score={s:.3f}  | {d['snippet']}")
