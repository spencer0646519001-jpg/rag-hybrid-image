import torch
from torch.nn import TransformerEncoder, TransformerEncoderLayer

# å›ºå®šäº‚æ•¸ç¨®å­ï¼ˆè®“æ¯æ¬¡åˆå§‹åŒ–ä¸€è‡´ï¼‰
torch.manual_seed(42)

# å‡è¨­æˆ‘å€‘æœ‰ 4 ç¶­ç‰¹å¾µçš„åºåˆ—ï¼Œç¸½å…± 5 å€‹æ™‚é–“é»ï¼ˆé¡ä¼¼èªéŸ³æˆ–å¥å­ä¸­çš„å­—ï¼‰
# å½¢ç‹€ï¼š[åºåˆ—é•·åº¦, æ‰¹æ¬¡å¤§å°, ç‰¹å¾µç¶­åº¦]
x = torch.randn(600, 1, 4)  

print("ğŸ“¥ è¼¸å…¥å‘é‡ xï¼š")
print(x.squeeze(1))

# å®šç¾©ä¸€å±¤ Transformer Encoder Layerï¼ˆå« Multi-Head Attentionï¼‰
encoder_layer = TransformerEncoderLayer(
    d_model=4,     # æ¯å€‹å‘é‡ç¶­åº¦ï¼ˆç‰¹å¾µæ•¸ï¼‰
    nhead=2        # æ³¨æ„åŠ›é ­æ•¸ï¼ˆåˆ†æˆå¹¾çµ„çœ‹å½¼æ­¤ï¼‰
)

# çµ„æˆå®Œæ•´çš„ Encoder æ¨¡çµ„ï¼ˆé€™è£¡åªç”¨ 1 å±¤ï¼‰
transformer_encoder = TransformerEncoder(encoder_layer, num_layers=3)

# å‰å‘å‚³æ’­
output = transformer_encoder(x)

print("\nğŸ“¤ ç¶“é Transformer Encoder å¾Œçš„è¼¸å‡ºå‘é‡ï¼š")
print(output.squeeze(1))
